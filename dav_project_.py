# -*- coding: utf-8 -*-
"""DAV Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NoWQRFlevprAyjcprEaNQxvX_Pz9jJkQ
"""

# Step 1: Unzip the file
!unzip accident_prediction_dataset.zip -d accident_prediction_dataset

# Step 2: Verify contents
!ls accident_prediction_dataset

# Commented out IPython magic to ensure Python compatibility.
# %cd accident_prediction_dataset/

!ls

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df=pd.read_csv("accident_prediction_india.csv")

#dropping duplicates
df.drop_duplicates()

for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)
    else:
        df[col].fillna(df[col].median(), inplace=True)

categorical_cols = ['Location', 'Weather', 'Road_Type', 'Severity']
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].astype('category')

print("\n--- Dataset Overview ---")
print(df.info())
print(df.describe(include='all'))

if 'Time' in df.columns:
    df['Hour'] = df['Time'].dt.hour
    plt.figure(figsize=(8,4))
    sns.countplot(x='Hour', data=df, palette='crest')
    plt.title("Accident Frequency by Hour")
    plt.xlabel("Hour of the Day")
    plt.ylabel("Count")
    plt.show()

if 'Date' in df.columns:
    df['Day'] = df['Date'].dt.day_name()
    plt.figure(figsize=(8,4))
    sns.countplot(x='Day', data=df, order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
    plt.title("Accidents by Day of Week")
    plt.xlabel("Day")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.show()
if 'Weather' in df.columns and 'Severity' in df.columns:
    plt.figure(figsize=(8,5))
    sns.countplot(x='Weather', hue='Severity', data=df, palette='coolwarm')
    plt.title("Accidents by Weather & Severity")
    plt.xticks(rotation=45)
    plt.show()

if {'Latitude', 'Longitude'}.issubset(df.columns):
    plt.figure(figsize=(8,6))
    sns.kdeplot(
        data=df, x='Longitude', y='Latitude', fill=True, cmap='Reds', thresh=0.05
    )
    plt.title("Accident Hotspots in India (Density Map)")
    plt.show()

plt.figure(figsize=(8,6))
sns.heatmap(df.select_dtypes('number').corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

"""ML part

"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv("accident_prediction_india.csv")
print(df.head())
print(df.info())

df = df.fillna(df.mode().iloc[0])
df = df.drop(columns=['Accident Location Details', 'City Name', 'State Name'])

label_cols = [
    'Month', 'Day of Week', 'Time of Day', 'Vehicle Type Involved',
    'Weather Conditions', 'Road Type', 'Road Condition', 'Lighting Conditions',
    'Traffic Control Presence', 'Driver Gender', 'Driver License Status',
    'Alcohol Involvement'
]

le = LabelEncoder()
for col in label_cols:
    df[col] = le.fit_transform(df[col])

X = df.drop(columns=['Accident Severity'])
y = df['Accident Severity']
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42,
    class_weight='balanced'
)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print("✅ Accuracy:", round(accuracy_score(y_test, y_pred), 3))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

importances = pd.Series(rf.feature_importances_, index=X.columns)
plt.figure(figsize=(10,6))
importances.sort_values(ascending=True).plot(kind='barh')
plt.title('Top Features Influencing Accident Severity')
plt.show()

import joblib
joblib.dump(rf, "random_forest_accident_model.pkl")

print(df.columns)

from IPython.display import display
from ipywidgets import interact

def show_accidents(day):
    subset = df[df['Day of Week'] == day]
    counts = (
        subset.groupby('Accident Severity')
        .size()
        .reset_index(name='Accident Count')
    )

    print(f"Accidents on {day} by severity:")
    display(counts)

interact(show_accidents, day=sorted(df['Day of Week'].unique()))

print(df.columns.tolist())

df = pd.read_csv("accident_prediction_india.csv", encoding='utf-8', sep=',', engine='python')
print(df.columns.tolist())

import pandas as pd
import folium
from ipywidgets import interact

def show_accidents(day):
    subset = df[df['Day of Week'] == day]
    city_counts = subset.groupby(['City Name', 'State Name']).size().reset_index(name='Accident Count')
    print(f"Accident counts for {day}:")
    display(city_counts.sort_values('Accident Count', ascending=False))

interact(show_accidents, day=sorted(df['Day of Week'].unique()))

import json
import requests

# ✅ Use the URL as a string inside quotes
geo_url = "https://raw.githubusercontent.com/geohacker/india/master/state/india_telengana.geojson"

# Download and parse the geojson data
geo_data = requests.get(geo_url).json()

# Quick check
print(type(geo_data))

import folium
from ipywidgets import interact

# Example: aggregate accident counts by state and day
state_day = df.groupby(['State Name', 'Day of Week']).size().reset_index(name='Accidents')

def plot_day(day):
    subset = state_day[state_day['Day of Week'] == day]
    m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)

    folium.Choropleth(
        geo_data=geo_data,
        data=subset,
        columns=['State Name', 'Accidents'],
        key_on='feature.properties.NAME_1',  # property in the GeoJSON file
        fill_color='YlOrRd',
        fill_opacity=0.7,
        line_opacity=0.2,
        legend_name='Number of Accidents'
    ).add_to(m)

    display(m)

interact(plot_day, day=sorted(df['Day of Week'].unique()))